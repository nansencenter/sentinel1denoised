{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from s1denoise import Sentinel1Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified: 27/10/2020 by AK\n",
    "#\n",
    "# In this quick start practical tutorial, you will learn how to get the power factors from S1 Level-1 GRD data.\n",
    "# The processing comprises two stages. 1st is related to the method called 'experiment_powerBalancing'\n",
    "# of the 'Sentinel1Image' class to obtain statistics for an each individual training file.\n",
    "# The aggregated statistics processing is the second stage of the method and implemnted as a\n",
    "# python script called 'analyze_experiment_powerBalancingParameters.py'.\n",
    "# In this tutorial we follow the mentioned implementation almost line by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-requerments: It is assumed that you have dowmloaded and installed Sentinel1Denoised package:\n",
    "# https://github.com/nansencenter/sentinel1denoised/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Stage 1. Processing of individual training files (S1 Level-1 GRD)  #\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First you need to import Sentinel1Image class and open \n",
    "# a S1 Level1 GRD file (IW GRDH in our case, replace with yours)\n",
    "# Note: In our example we process the only one training file but in real data processing you need to use tens of files\n",
    "# in batch manner\n",
    "\n",
    "# Define filename and polarization of the data we want to process\n",
    "input_file = '/data1/antonk/tmp/S1B_EW_GRDM_1SDH_20190810T164042_20190810T164124_017526_020F60_7343.SAFE'\n",
    "polarization = 'HV'\n",
    "\n",
    "#input_file = '/mnt/sverdrup-2/sat_auxdata/denoise/dolldrums/zip/S1A_IW_GRDH_1SDV_20200607T075151_20200607T075220_032908_03CFD7_9E14.zip'\n",
    "#polarization = 'VH'\n",
    "\n",
    "s1 = Sentinel1Image(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We further go almost in line by line manner through the method called 'experiment_powerBalancing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip size of side pixels, ~1km\n",
    "cPx = {'IW':100, 'EW':25}[s1.obsMode]\n",
    "num_swaths = {'IW':3, 'EW':5}[s1.obsMode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call 'subswathIndexMap' method to get matrix with sub-swath numbers\n",
    "# consistent with the data grid\n",
    "subswathIndexMap = s1.subswathIndexMap(polarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualize the sub-swath numbers\n",
    "# Color corresponds to different sub-swath. In our case:\n",
    "# blue - 1, green - 2, yellow - 3, black - no data\n",
    "plt.clf()\n",
    "plt.imshow(subswathIndexMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get matrices of calibrated raw sigma zero and NESZ from ESA-provided annotaion files\n",
    "sigma0 = s1.rawSigma0Map(polarization)\n",
    "rawNoiseEquivalentSigma0 = s1.rawNoiseEquivalentSigma0Map(polarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the noise scaling coefficients and scale the noise\n",
    "# to a matrix called 'noiseEquivalentSigma0'\n",
    "# The scaling is applied separetely for each sub-swath\n",
    "\n",
    "noiseEquivalentSigma0 = np.zeros_like(rawNoiseEquivalentSigma0)\n",
    "noiseScalingParameters = s1.import_denoisingCoefficients(polarization)[0]\n",
    "\n",
    "for iSW in range(1, num_swaths+1):\n",
    "    swathname = '%s%s' % (s1.obsMode, iSW)\n",
    "    scale = noiseScalingParameters[swathname]\n",
    "    print(swathname, scale)\n",
    "    valid = (subswathIndexMap==iSW)\n",
    "    noiseEquivalentSigma0[valid] = rawNoiseEquivalentSigma0[valid] * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the line indexes where we have data for each element in range ditrection\n",
    "validLineIndices = np.argwhere(np.sum(subswathIndexMap!=0,axis=1)==s1.shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of lines to averge (block height), in our case 1000 px\n",
    "# and calculate the block bounds\n",
    "numberOfLinesToAverage = 1000\n",
    "blockBounds = np.arange(validLineIndices.min(), validLineIndices.max(),\n",
    "                                numberOfLinesToAverage, dtype='uint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out block bounds coordinates (pixel indexes)\n",
    "blockBounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a dictonary for results for each sub-block\n",
    "# sigma0 is a vector with the mean range profile\n",
    "# noiseEquivalentSigma0 is a vector with the mean range profile of ESA-provided noise\n",
    "# balancingPower is a value of intermediate balancing power\n",
    "# correlationCoefficient is a value of correlation coefficient between raw sigma0 and the scaled noise\n",
    "# fitResidual is a value of optimization residual\n",
    "results = { '%s%s' % (s1.obsMode, li): defaultdict(list) for li in range(1, num_swaths+1) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over blocks\n",
    "for iBlk in range(len(blockBounds)-1):   \n",
    "    # Slicing 2D arrays for each block\n",
    "    # sigma0\n",
    "    blockS0 = sigma0[blockBounds[iBlk]:blockBounds[iBlk+1],:]   \n",
    "    # Scaled noise\n",
    "    blockN0 = noiseEquivalentSigma0[blockBounds[iBlk]:blockBounds[iBlk+1],:]\n",
    "    # Raw ESA noise\n",
    "    blockRN0 = rawNoiseEquivalentSigma0[blockBounds[iBlk]:blockBounds[iBlk+1],:]\n",
    "    # Sub-swath indices for the block\n",
    "    blockSWI = subswathIndexMap[blockBounds[iBlk]:blockBounds[iBlk+1],:]\n",
    "    \n",
    "    # An arbitrary empirical adjustment.\n",
    "    # When the number of valid range pixels is too small, it is hard to get reliable estimation.\n",
    "    # By allowing the use of some pixels with negative values, the number of range pixels to be used \n",
    "    # for fitting increases.\n",
    "    # In short, it can be any value but we set it as 0.5 for no special reason.\n",
    "    pixelValidity = (np.nanmean(blockS0 - blockRN0 * 0.5, axis=0) > 0) \n",
    "    if pixelValidity.sum() <= (blockS0.shape[1] * 0.9):\n",
    "        continue\n",
    "\n",
    "    fitCoefficients = []\n",
    "    \n",
    "    # Loop over sub-blocks\n",
    "    for iSW in range(1, num_swaths+1):\n",
    "        subswathID = '%s%s' % (s1.obsMode, iSW)\n",
    "        # Get valid pixel indices based on 'pixelValidity'\n",
    "        pixelIndex = np.nonzero((blockSWI==iSW).sum(axis=0) * pixelValidity)[0][cPx:-cPx]\n",
    "        if pixelIndex.sum()==0:\n",
    "            continue\n",
    "            \n",
    "        # Compute vectors with mean range profiles for sub-block by averaging in azimuth direction\n",
    "        meanS0 = np.nanmean(np.where(blockSWI==iSW, blockS0, np.nan), axis=0)[pixelIndex]\n",
    "        meanN0 = np.nanmean(np.where(blockSWI==iSW, blockN0, np.nan), axis=0)[pixelIndex]\n",
    "        meanRN0 = np.nanmean(np.where(blockSWI==iSW, blockRN0, np.nan), axis=0)[pixelIndex]\n",
    "        \n",
    "        # Do fitting for a difference between sigma0 and scaled noise\n",
    "        fitResults = np.polyfit(pixelIndex, meanS0 - meanN0, deg=1, full=True)\n",
    "        \n",
    "        # Store slope value for further calculations\n",
    "        fitCoefficients.append(fitResults[0])\n",
    "        \n",
    "        # Store results\n",
    "        results[subswathID]['sigma0'].append(meanS0)\n",
    "        results[subswathID]['noiseEquivalentSigma0'].append(meanRN0)\n",
    "        results[subswathID]['correlationCoefficient'].append(np.corrcoef(meanS0, meanN0)[0,1])\n",
    "        results[subswathID]['fitResidual'].append(fitResults[1].item())\n",
    "        results[subswathID]['noiseEquivalentSigma0Scaled'].append(meanN0)\n",
    "        results[subswathID]['pixelIndex'].append(pixelIndex)\n",
    "        results[subswathID]['fitResults'].append(fitResults)\n",
    "    \n",
    "    # Initialize a vector with zeros for each sub-swath \n",
    "    balancingPower = np.zeros(num_swaths)\n",
    "\n",
    "    # Loop over sub-swath margins\n",
    "    for li in range(len(s1.import_swathBounds(polarization))-1):\n",
    "        # Calculate pixel coordinate of interswath boundary\n",
    "        interswathBounds = (np.where(np.gradient(blockSWI, axis=1) == 0.5)[1]\n",
    "                            .reshape((num_swaths - 1) * numberOfLinesToAverage, 2)[li::4].mean())\n",
    "        # Compute power value left to a boundary as slope*interswathBounds + residual coef.\n",
    "        power1 = fitCoefficients[li][0] * interswathBounds + fitCoefficients[li][1]\n",
    "        # Compute power value right to a boundary as slope*interswathBounds + residual coef.\n",
    "        power2 = fitCoefficients[li+1][0] * interswathBounds + fitCoefficients[li+1][1]\n",
    "        # Compute balancing power vector as a power difference between the powers in neighboring sub-swaths\n",
    "        balancingPower[li+1] = power2 - power1\n",
    "        \n",
    "    # Compute cumulative sum vector of of the balancing power near sub-swath margins\n",
    "    balancingPower = np.cumsum(balancingPower)\n",
    "\n",
    "    # Loop through sub-swaths to offset noise power for the each one\n",
    "    for iSW in range(1, num_swaths+1):\n",
    "        valid = (blockSWI==iSW)\n",
    "        blockN0[valid] += balancingPower[iSW-1]\n",
    "\n",
    "    # Value of the mean of the difference bewtween noise matrices for block across sub-swaths >=2\n",
    "    powerBias = np.nanmean((blockRN0-blockN0)[blockSWI>=2])\n",
    "    \n",
    "    # Add the mean to the balancing power vector\n",
    "    balancingPower += powerBias\n",
    "\n",
    "    for iSW in range(1, num_swaths+1):\n",
    "        results['%s%s' % (s1.obsMode, iSW)]['balancingPower'].append(balancingPower[iSW-1])\n",
    "\n",
    "# Save results for individual file in NPZ format\n",
    "np.savez(s1.name.split('.')[0] + '_powerBalancing.npz', **results)\n",
    "\n",
    "# At this point we obtained statistics for many files and ready for the 2nd stage that is\n",
    "# aggregated statistics processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot range profiles of sigma zero, NESZ, denoised signal, and approximated denoised signal in one sub-swath\n",
    "\n",
    "j = 0                        # sub-block index\n",
    "i = list(results.keys())[0]  # sub-swath name\n",
    "\n",
    "# scaled denoised signal\n",
    "meanS0ij = results[i]['sigma0'][j] - results[i]['noiseEquivalentSigma0Scaled'][j]\n",
    "\n",
    "# approximated signal\n",
    "meanS0ij_fit = np.polyval(results[i]['fitResults'][j][0], results[i]['pixelIndex'][j])\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(results[i]['pixelIndex'][j], results[i]['sigma0'][j], label='$\\sigma^0$')\n",
    "plt.plot(results[i]['pixelIndex'][j], results[i]['noiseEquivalentSigma0'][j], label='$\\sigma^0_N$')\n",
    "plt.plot(results[i]['pixelIndex'][j], results[i]['noiseEquivalentSigma0Scaled'][j], label='$\\sigma^0_{NS}$')\n",
    "plt.plot(results[i]['pixelIndex'][j], meanS0ij, label='$\\sigma^0 - \\sigma^0_{NS}$')\n",
    "plt.plot(results[i]['pixelIndex'][j], meanS0ij_fit, 'k-', label='$f(c_0, c_1, c)$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot range profiles of denoised signal, and approximated denoised signal in two sub-swaths\n",
    "j = 0                          # sub-block index\n",
    "i1 = list(results.keys())[0]   # sub-swath name\n",
    "i2 = list(results.keys())[1]   # sub-swath name\n",
    "\n",
    "# denoised_signal\n",
    "meanS01j = results[i1]['sigma0'][j] - results[i1]['noiseEquivalentSigma0Scaled'][j]\n",
    "meanS02j = results[i2]['sigma0'][j] - results[i2]['noiseEquivalentSigma0Scaled'][j]\n",
    "\n",
    "# approximated signal\n",
    "meanS01j_fit = np.polyval(results[i1]['fitResults'][j][0], results[i1]['pixelIndex'][j])\n",
    "meanS02j_fit = np.polyval(results[i2]['fitResults'][j][0], results[i2]['pixelIndex'][j])\n",
    "\n",
    "# approximated signal at boundary\n",
    "c_left = results[i1]['pixelIndex'][j][-1]\n",
    "p1_left =  np.polyval(results[i1]['fitResults'][j][0], c_left)\n",
    "c_right = results[i2]['pixelIndex'][j][0]\n",
    "p2_right = np.polyval(results[i2]['fitResults'][j][0], c_right)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(results[i1]['pixelIndex'][j], meanS01j, label='$\\sigma^0_1 - \\sigma^0_{NS1}$')\n",
    "plt.plot(results[i2]['pixelIndex'][j], meanS02j, label='$\\sigma^0_2 - \\sigma^0_{NS2}$')\n",
    "plt.plot(results[i1]['pixelIndex'][j], meanS01j_fit, 'k-', label='$f(c_{01}, c_{11}, c)$')\n",
    "plt.plot(results[i2]['pixelIndex'][j], meanS02j_fit, 'k-', label='$f(c_{02}, c_{12}, c)$')\n",
    "\n",
    "plt.plot([c_left, c_right], [p1_left, p1_left], 'ko--', label='$p_1^l$')\n",
    "plt.plot([c_left, c_right], [p2_right, p2_right], 'kx--', label='$p_2^r$')\n",
    "\n",
    "plt.xlim([2800, 3200])\n",
    "plt.ylim([0.00105, 0.00135])\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Stage 2. Processing of aggregated statistics\n",
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we have statistics for the training files we may start to get final results on the power balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists for results from each npz file\n",
    "powerDifference = []\n",
    "balancingPower = []\n",
    "correlationCoefficient = []\n",
    "fitResidual = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of npz files with individual statistics\n",
    "npzFiles = glob.glob('*powerBalancing*.npz')\n",
    "\n",
    "# Loop over the files\n",
    "for npzFile in npzFiles:\n",
    "    print('importing %s' % npzFile)\n",
    "    npz = np.load(npzFile)\n",
    "    npz.allow_pickle = True\n",
    "    # Number of sub-blocks in sub-swath\n",
    "    numberOfSubblocks = np.unique([ len(npz['%s%s' % (s1.obsMode,iSW)].item()['balancingPower'])\n",
    "                                    for iSW in range(1,({'IW': 3, 'EW': 5}[s1.obsMode]+1)) ])\n",
    "    \n",
    "    # Check if a number of sub-blocks are not consistent for all subswaths\n",
    "    if numberOfSubblocks.size != 1:\n",
    "        print('*** numberOfSubblocks are not consistent for all subswaths.')\n",
    "        continue\n",
    "    \n",
    "    # Unpack the number of sub-blocks\n",
    "    numberOfSubblocks = numberOfSubblocks.item()\n",
    "    \n",
    "    # Loop over each sub-block\n",
    "    for li in range(numberOfSubblocks):\n",
    "        # Values of intermediate sigma0\n",
    "        powerDifference.append([\n",
    "              np.nanmean(10*np.log10(npz['%s%s' % (s1.obsMode,iSW)].item()['sigma0'][li]))\n",
    "            - np.nanmean(10*np.log10(npz['%s%s' % (s1.obsMode,iSW)].item()['noiseEquivalentSigma0'][li]))\n",
    "            for iSW in range(1,({'IW': 3, 'EW': 5}[s1.obsMode])+1) ])\n",
    "        \n",
    "        # Values of balancing power\n",
    "        balancingPower.append([\n",
    "            npz['%s%s' % (s1.obsMode,iSW)].item()['balancingPower'][li]\n",
    "            for iSW in range(1,({'IW': 3, 'EW': 5}[s1.obsMode])+1) ])\n",
    "        \n",
    "        # Values of balancing correlation coefficient\n",
    "        correlationCoefficient.append([\n",
    "            npz['%s%s' % (s1.obsMode,iSW)].item()['correlationCoefficient'][li]\n",
    "            for iSW in range(1,({'IW': 3, 'EW': 5}[s1.obsMode])+1) ])\n",
    "        \n",
    "        # Values of residuals\n",
    "        fitResidual.append([\n",
    "            npz['%s%s' % (s1.obsMode,iSW)].item()['fitResidual'][li]\n",
    "            for iSW in range(1,({'IW': 3, 'EW': 5}[s1.obsMode])+1) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the loop, convert the aggregated lists to Numpy arrays\n",
    "powerDifference = np.array(powerDifference)\n",
    "balancingPower = np.array(balancingPower)\n",
    "correlationCoefficient = np.array(correlationCoefficient)\n",
    "fitResidual = np.array(fitResidual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we ready to compute fit values for each sub-swath based on aggregated statistics which are the power balancing factors\n",
    "# and their confident intervals\n",
    "powerBalancingParameters = {'%s%s' % (s1.obsMode,li): {} for li in range(1,({'IW': 3, 'EW': 5}[s1.obsMode]+1))}\n",
    "powerBalancingParametersRMSE = {'%s%s' % (s1.obsMode,li): {} for li in range(1,({'IW': 3, 'EW': 5}[s1.obsMode]+1))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute vectors which are the mean across all sub-swath for each sub-block\n",
    "pd = np.mean(powerDifference, axis=1)\n",
    "cc = np.min(correlationCoefficient, axis=1)\n",
    "fr = np.max(fitResidual, axis=1)\n",
    "# Vector of weight factor for fitting\n",
    "w = cc / fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over sub-swaths for fitting vectors of mean values of power difference across all sub-swaths\n",
    "# with balanacing power vector for a given sub-swath\n",
    "for iSW in range(1,({'IW': 3, 'EW': 5}[s1.obsMode])+1):\n",
    "    bp = balancingPower[:,iSW-1]\n",
    "    fitResults = np.polyfit(pd, bp, deg=0, w=w)\n",
    "    powerBalancingParameters['%s%s' % (s1.obsMode,iSW)] = fitResults[0]\n",
    "    powerBalancingParametersRMSE['%s%s' % (s1.obsMode,iSW)] = np.sqrt(np.sum((fitResults[0]-bp)**2 * w) / np.sum(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we finished with the whole processing chain and obtained the power balancing factors for each sub-swath\n",
    "# and their RMSE (can be considered as confident inetrval for the factor)\n",
    "print('Power balancing factors:\\n%s' % powerBalancingParameters)\n",
    "print('\\nRMSE of the power balancing factors:\\n%s' % powerBalancingParametersRMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
